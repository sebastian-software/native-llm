<p align="center">
  <img src="https://raw.githubusercontent.com/sebastian-software/native-llm/main/.github/logo.svg" width="120" alt="native-llm logo" />
</p>

<h1 align="center">native-llm</h1>

<p align="center">
  <strong>Run AI models locally. No cloud. No limits. No cost.</strong>
</p>

<p align="center">
  <a href="https://github.com/sebastian-software/native-llm/actions/workflows/ci.yml"><img src="https://github.com/sebastian-software/native-llm/actions/workflows/ci.yml/badge.svg" alt="CI"></a>
  <a href="https://codecov.io/gh/sebastian-software/native-llm"><img src="https://codecov.io/gh/sebastian-software/native-llm/graph/badge.svg" alt="codecov"></a>
  <a href="https://www.npmjs.com/package/native-llm"><img src="https://img.shields.io/npm/v/native-llm.svg" alt="npm version"></a>
  <a href="https://www.npmjs.com/package/native-llm"><img src="https://img.shields.io/npm/dm/native-llm.svg" alt="npm downloads"></a>
  <a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-blue.svg" alt="License: MIT"></a>
</p>

<p align="center">
  <a href="#-quick-start">Quick Start</a> â€¢
  <a href="#-why-native-llm">Why native-llm</a> â€¢
  <a href="#-models">Models</a> â€¢
  <a href="https://sebastian-software.github.io/native-llm/">Documentation</a>
</p>

---

## ğŸ¯ Why native-llm?

|             | â˜ï¸ Cloud AI              | ğŸ  native-llm        |
| ----------- | ------------------------ | -------------------- |
| **Cost**    | $0.001 - $0.10 per query | **Free forever**     |
| **Speed**   | 1-20 seconds             | **< 100ms**          |
| **Privacy** | Data sent to servers     | **100% local**       |
| **Limits**  | Rate limits & quotas     | **Unlimited**        |
| **Offline** | âŒ Requires internet     | âœ… **Works offline** |

**The bottom line:** Local models now achieve **91% of GPT-5's quality** â€” at zero cost.

---

## ğŸš€ Quick Start

```bash
npm install native-llm
```

```typescript
import { LLMEngine } from "native-llm"

// That's it. One line to load a model.
const engine = new LLMEngine({ model: "gemma" })

const result = await engine.generate({
  prompt: "Explain quantum computing to a 5-year-old"
})

console.log(result.text)
// â†’ "Imagine you have a magical coin that can be heads AND tails at the same time..."
```

Models download automatically on first use. No setup. No configuration. Just works.

---

## âš¡ Performance

Benchmarked on **Apple M1 Ultra** with Metal GPU acceleration:

| Model                 | Size  | Speed        | Best For          |
| --------------------- | ----- | ------------ | ----------------- |
| ğŸš€ **Gemma 3n E2B**   | 3 GB  | **36 tok/s** | Maximum speed     |
| â­ **Gemma 3n E4B**   | 5 GB  | **18 tok/s** | Best balance      |
| ğŸ’» **Qwen 2.5 Coder** | 5 GB  | **23 tok/s** | Code generation   |
| ğŸ§  **DeepSeek R1**    | 5 GB  | **9 tok/s**  | Complex reasoning |
| ğŸ‘‘ **Gemma 3 27B**    | 18 GB | **5 tok/s**  | Maximum quality   |

> ğŸ’¡ **Our pick:** Start with `gemma-3n-e4b` â€” it's the sweet spot of quality and speed.

---

## ğŸ¨ Models

Use simple aliases â€” we handle the rest:

```typescript
new LLMEngine({ model: "gemma" }) // Fast & efficient
new LLMEngine({ model: "gemma-large" }) // Maximum quality
new LLMEngine({ model: "qwen-coder" }) // Code generation
new LLMEngine({ model: "deepseek" }) // Chain-of-thought reasoning
new LLMEngine({ model: "phi" }) // STEM & science
```

Or use any of the **1000+ GGUF models** on HuggingFace:

```typescript
new LLMEngine({ model: "/path/to/any-model.gguf" })
```

---

## âœ¨ Features

| Feature               | Description                                                 |
| --------------------- | ----------------------------------------------------------- |
| ğŸ”¥ **Native Speed**   | Direct N-API bindings to llama.cpp â€” no subprocess overhead |
| ğŸ **Metal GPU**      | Full Apple Silicon acceleration out of the box              |
| ğŸ–¥ï¸ **Cross-Platform** | macOS, Linux, Windows â€” CUDA support for NVIDIA             |
| ğŸ“¦ **Auto-Download**  | Models fetched from HuggingFace automatically               |
| ğŸŒŠ **Streaming**      | Real-time token-by-token output                             |
| ğŸ“ **TypeScript**     | Full type definitions included                              |

---

## ğŸ”‘ Setup for Gemma Models

Gemma models require a free HuggingFace token:

```bash
export HF_TOKEN="hf_your_token_here"
```

Get yours in 30 seconds: [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)

---

## ğŸ“š Documentation

**[â†’ Full Documentation](https://sebastian-software.github.io/native-llm/)** â€” Benchmarks, model
comparison, streaming, chat API, and more.

---

## ğŸ’– Credits

Built on the shoulders of giants:

- [llama.cpp](https://github.com/ggerganov/llama.cpp) â€” The inference engine that makes this
  possible
- [node-llama-cpp](https://github.com/withcatai/node-llama-cpp) â€” Excellent Node.js bindings
- [bartowski](https://huggingface.co/bartowski) â€” High-quality GGUF quantizations

---

<p align="center">
  <strong>MIT License</strong> Â· Made with â¤ï¸ by <a href="https://sebastian-software.de">Sebastian Software</a>
</p>
